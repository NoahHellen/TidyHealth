---
title: "TidyEducation"
output:
 html_document:
   toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load packages

```{r}
library(naniar)
library(tidyverse)
```

# Foreword

Before proceeding, the **probabilistic** nature of the dataset must be addressed.

$$\boxed{\textit{Statistical inference necessitates probabilistic assumptions}}$$

In this dataset, each covariate can be assumed to follow a probability distribution.

# Exploration

In this section, we perform exploratory data analysis.

### Load dataset

First, let's load the dataset.

```{r}
totaledu <- read.csv("data/edu.csv")
```

Next, let's select a subset of relevant variables.

```{r}
edu <- totaledu %>%
  # Choose relevant variables.
  select(
    EstablishmentTypeGroup..code.,
    EstablishmentStatus..code.,
    Boarders..code.,
    NumberOfPupils,
    PercentageFSM,
    Town,
    Gender..code.,
    PercentageFSM
  ) %>%
  # Remove empty `Town` entries.
  filter(Town != "") %>%
  # Remove encoding error.
  filter(row_number() != 47968) %>%
  # Normalise naming.
  mutate(Town = tolower(Town))
```

Let's also remove `closed` schools.

```{r}
edu <- edu %>%
  filter(
    EstablishmentStatus..code. == 1
  ) %>%
  select(
    -EstablishmentStatus..code.
  )
```

Finally, let's create a code for the variable `Town`.

```{r}
town_code <- edu %>%
  select(Town) %>%
  unique() %>%
  mutate(TownCode = row_number() - 1)
```

Let's now impute this code into the tibble.

```{r}
edu <- edu %>%
  left_join(town_code, by = "Town") %>%
  select(-Town)
```

### Data types

Let's establish the data types of this dataset.

```{r}
head(edu)
```

* **Nominal**
  * TypeOfEstablishment
  * Boarders
  * Town
  * Gender
* **Orindal**
  * NA
* **Interval**
  * NA
* **Ratio**
  * NumberOfPupils
  * PercentageFSM

# Data quality

In this section, we will focus on missing and anomalous data.

### Missing data

Let's first assess the proportion of missing data; a large proportion might suggest a poor quality dataset.

```{r}
gg_miss_case(edu)
```

Given the dataset's size, these results imply only a small impact on its quality.

### MCAR, MAR, MNAR

Let's identify the types of missingness present in the data.

There are formal definitions for missingness \(1).

- **MCAR**
    - $P(M|D) = P(M)$
- **MAR**
    - $P(M|D) = P(M|D_{obs})$
- **MNAR**
    - $P(M|D) = P(M|D)$

There are also informal methods for diagnosing missingness.

- Little's MCAR test
- Diagnostic plots

Let's first use Little's MCAR test.

```{r}
mcar_test(edu)
```

The $p$-value $0 < 0.05$ suggests the data is **not** MCAR.

We can now use some diagnostic plots to help visualise the missingness.

```{r}
ggplot(
  edu,
  aes(x = NumberOfPupils, y = PercentageFSM)
) +
  geom_miss_point() +
  facet_wrap(~EstablishmentTypeGroup..code.)
```

In the following, we regress covariates with missingness present in `PercentageFSM` and `NumberOfPupils` to see if there is a relationship.

```{r}
missing_fsm <- edu$PercentageFSM
missing_fsm <- ifelse(is.na(missing_fsm), 1, 0)
data_matrix <- edu %>%
  select(-PercentageFSM) %>%
  select(-NumberOfPupils) %>%
  as.matrix()
p_vals <- rep(NA, ncol(data_matrix))
for (j in 1:ncol(data_matrix)) {
  s <- summary(glm(missing_fsm ~ data_matrix[, j]),
    family = binomial
  )
  p_vals[j] <- s$coefficients[2, 4]
}

p_vals
```

```{r}
missing_pupils <- edu$NumberOfPupils
missing_pupils <- ifelse(is.na(missing_pupils), 1, 0)
data_matrix <- edu %>%
  select(-NumberOfPupils) %>%
  select(-PercentageFSM) %>%
  as.matrix()
p_vals <- rep(NA, ncol(data_matrix))
for (j in 1:ncol(data_matrix)) {
  s <- summary(glm(missing_pupils ~ data_matrix[, j]),
    family = binomial
  )
  p_vals[j] <- s$coefficients[2, 4]
}

p_vals
```

The `p_vals` are significant in both cases, which suggests the data is MAR.

### Handling missing data

The method of handling missing data depends on the type of missingness.

We have established the data is likely to be MAR, therefore the method of **multiple imputation** can be used.

### Outliers

# Statistical inference

In this section, we explore the statistical properties of the dataset.

In [Central tendency](#central-tendency) and [Spread](#spread), we will derive the empirical **mean** and **variance** of `PercentageFSM` without making use of any probabilistic assumptions.

That is, we will explore the central tendency and spread of `PercentageFSM`.

In [Skewness](#skewness) we make use of the **probabilistic** nature of the covariates.

### Central tendency

To find the central tendency of `PercentageFSM`, we denote $\mu$ as the central value and minimise

\[
  SSD(\mu) = \sum_{i=1}^n (x_i-\mu)^2,
\]

where $SSD$ is the *sum of squared deviations from $\mu$*.

```{r}
schools_fsm <- edu$PercentageFSM %>%
  na.omit() %>%
  as.vector()

is.vector(schools_fsm)

ssd <- function(mu) {
  sum((schools_fsm - mu)^2)
}
mu <- 20:35
plot(mu,
  lapply(mu, ssd),
  "l",
  main = "Central tendency measure",
  xlab = "Central tendency",
  ylab = "SSD"
)
```

It is clear from the plot that the centre of the data lies in the interval $[25,30]$.

*Note that missing values were simply dropped. This will be addressed in [Data quality](#data-quality).*

### Spread

To find spread, we simply compute the empirical variance

\[
  var(x_1, \cdots, x_n) = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2.
\]

```{r}
variance <- function(mu) {
  1 / length(schools_fsm) * sum((schools_fsm - mu)^2)
}

cat("The spread of `schools_fsm` is", variance(25.53))
```


### Skewness

An estimate for skewness is given by

\[
  g_1 = \frac{m_3}{m_2^{3/2}}, \hspace{5pt} b_1 = (\frac{n-1}{n})^\frac{3}{2} \cdot g_1,
\]

where $m_p$ is the $p$-th order centralised sample moment $m_p = n^{-1} \sum_{i=1}^n(x_i - \bar{x})^p$
and $b_1$ is Bessel's correction.

Let's apply this to `PercentageFSM`.

```{r}
skew_estimate <- function(data, bessel = FALSE) {
  mu1 <- mean(data)
  mu3 <- mean((data - mu1)^3)
  if (bessel) {
    denom <- variance(25.53)^(3 / 2)
  } else {
    denom <- ((length(data) - 1) * variance(25.53) / length(data))^(3 / 2)
  }
  mu3 / denom
}

b_1 <- skew_estimate(schools_fsm)

cat("The skewness of the data is", b_1)

hist(schools_fsm, 100)
```

The histogram validates the positive skewness observed from the estimate `b_1`.

# References

[1] King G, Honaker J, Joseph A, Scheve K. Analyzing Incomplete Political Science
Data: An Alternative Algorithm for Multiple Imputation. American Political
Science Review. 2001;95(1):49â€“69. pages 4
